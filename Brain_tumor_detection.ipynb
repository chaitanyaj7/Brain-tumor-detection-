{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25dbf0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "119c5009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your paths\n",
    "train_path = \"C:/library/Projects/I_P_project/Brain toumor detection by using cnn/Training\"\n",
    "test_path = \"C:/library/Projects/I_P_project/Brain toumor detection by using cnn/Testing\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85873817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation helps prevent overfitting\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1/255.,          # normalize pixel values\n",
    "    rotation_range=20,       # rotate images randomly\n",
    "    width_shift_range=0.2,   # shift horizontally\n",
    "    height_shift_range=0.2,  # shift vertically\n",
    "    shear_range=0.2,         # shear transformations\n",
    "    zoom_range=0.2,          # zoom\n",
    "    horizontal_flip=True,    # flip horizontally\n",
    "    fill_mode='nearest'      # fill empty pixels after transformations\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39cc1a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1/255.)  # only normalize test images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9da704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2870 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Flow from directories\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(150,150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "409d9b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 394 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_gen = test_datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(150,150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77139e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPILING THE MODEL\n",
    "\n",
    "# categorical_crossentropy → best for multi-class problems\n",
    "# Adam → efficient optimizer for CNN\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c5e5f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 394 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_gen = test_datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(150,150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "690e7efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    # Convolution + MaxPooling\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Flatten(),  # Flatten to feed into dense layers\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Prevent overfitting\n",
    "    Dense(4, activation='softmax')  # 4 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "702a92d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "89/89 [==============================] - 56s 612ms/step - loss: 1.3114 - accuracy: 0.3714 - val_loss: 1.6882 - val_accuracy: 0.1641\n",
      "Epoch 2/25\n",
      "89/89 [==============================] - 55s 613ms/step - loss: 1.1463 - accuracy: 0.4729 - val_loss: 2.1155 - val_accuracy: 0.2734\n",
      "Epoch 3/25\n",
      "89/89 [==============================] - 54s 603ms/step - loss: 1.0933 - accuracy: 0.5078 - val_loss: 2.4914 - val_accuracy: 0.2969\n",
      "Epoch 4/25\n",
      "89/89 [==============================] - 55s 614ms/step - loss: 0.9868 - accuracy: 0.5701 - val_loss: 2.6848 - val_accuracy: 0.3177\n",
      "Epoch 5/25\n",
      "89/89 [==============================] - 54s 607ms/step - loss: 0.9378 - accuracy: 0.5906 - val_loss: 3.4311 - val_accuracy: 0.3021\n",
      "Epoch 6/25\n",
      "89/89 [==============================] - 54s 608ms/step - loss: 0.9074 - accuracy: 0.6223 - val_loss: 3.4904 - val_accuracy: 0.3099\n",
      "Epoch 7/25\n",
      "89/89 [==============================] - 54s 601ms/step - loss: 0.8719 - accuracy: 0.6177 - val_loss: 3.9110 - val_accuracy: 0.3281\n",
      "Epoch 8/25\n",
      "89/89 [==============================] - 55s 617ms/step - loss: 0.8374 - accuracy: 0.6434 - val_loss: 3.5839 - val_accuracy: 0.3203\n",
      "Epoch 9/25\n",
      "89/89 [==============================] - 55s 617ms/step - loss: 0.8286 - accuracy: 0.6387 - val_loss: 2.8616 - val_accuracy: 0.3594\n",
      "Epoch 10/25\n",
      "89/89 [==============================] - 55s 612ms/step - loss: 0.7688 - accuracy: 0.6758 - val_loss: 2.7614 - val_accuracy: 0.3125\n",
      "Epoch 11/25\n",
      "89/89 [==============================] - 54s 606ms/step - loss: 0.7636 - accuracy: 0.6772 - val_loss: 3.5391 - val_accuracy: 0.3307\n",
      "Epoch 12/25\n",
      "89/89 [==============================] - 54s 605ms/step - loss: 0.7428 - accuracy: 0.6949 - val_loss: 3.5712 - val_accuracy: 0.3333\n",
      "Epoch 13/25\n",
      "89/89 [==============================] - 54s 609ms/step - loss: 0.7376 - accuracy: 0.6875 - val_loss: 3.2215 - val_accuracy: 0.3359\n",
      "Epoch 14/25\n",
      "89/89 [==============================] - 55s 614ms/step - loss: 0.7235 - accuracy: 0.6959 - val_loss: 3.3204 - val_accuracy: 0.3333\n",
      "Epoch 15/25\n",
      "89/89 [==============================] - 54s 610ms/step - loss: 0.6735 - accuracy: 0.7174 - val_loss: 2.9301 - val_accuracy: 0.3776\n",
      "Epoch 16/25\n",
      "89/89 [==============================] - 54s 601ms/step - loss: 0.7263 - accuracy: 0.6970 - val_loss: 4.2338 - val_accuracy: 0.3073\n",
      "Epoch 17/25\n",
      "89/89 [==============================] - 53s 593ms/step - loss: 0.6761 - accuracy: 0.7156 - val_loss: 2.7987 - val_accuracy: 0.3880\n",
      "Epoch 18/25\n",
      "89/89 [==============================] - 53s 595ms/step - loss: 0.6792 - accuracy: 0.7044 - val_loss: 3.1075 - val_accuracy: 0.3750\n",
      "Epoch 19/25\n",
      "89/89 [==============================] - 55s 618ms/step - loss: 0.6606 - accuracy: 0.7230 - val_loss: 3.5521 - val_accuracy: 0.3385\n",
      "Epoch 20/25\n",
      "89/89 [==============================] - 55s 615ms/step - loss: 0.6393 - accuracy: 0.7308 - val_loss: 3.6494 - val_accuracy: 0.3411\n",
      "Epoch 21/25\n",
      "89/89 [==============================] - 55s 617ms/step - loss: 0.6296 - accuracy: 0.7319 - val_loss: 3.6714 - val_accuracy: 0.3724\n",
      "Epoch 22/25\n",
      "89/89 [==============================] - 54s 608ms/step - loss: 0.6253 - accuracy: 0.7371 - val_loss: 3.2073 - val_accuracy: 0.3672\n",
      "Epoch 23/25\n",
      "89/89 [==============================] - 55s 619ms/step - loss: 0.5937 - accuracy: 0.7548 - val_loss: 3.2003 - val_accuracy: 0.3906\n",
      "Epoch 24/25\n",
      "89/89 [==============================] - 55s 615ms/step - loss: 0.6089 - accuracy: 0.7491 - val_loss: 4.5770 - val_accuracy: 0.3464\n",
      "Epoch 25/25\n",
      "89/89 [==============================] - 55s 618ms/step - loss: 0.5663 - accuracy: 0.7664 - val_loss: 4.5034 - val_accuracy: 0.3932\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=train_gen.samples // train_gen.batch_size,\n",
    "    validation_data=test_gen,\n",
    "    validation_steps=test_gen.samples // test_gen.batch_size,\n",
    "    epochs=25  # increase epochs if needed\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"brain_tumor_cnn_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6354a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
